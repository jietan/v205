---
title: Real-World Robot Learning with Masked Visual Pre-training
section: Oral
openreview: KWCZfuqshd
abstract: In this work, we explore self-supervised visual pre-training on images from
  diverse, in-the-wild videos for real-world robotic tasks. Like prior work, our visual
  representations are pre-trained via a masked autoencoder (MAE), frozen, and then
  passed into a learnable control module. Unlike prior work, we show that the pre-trained
  representations are effective across a range of real-world robotic tasks and embodiments.
  We find that our encoder consistently outperforms CLIP (up to 75%), supervised ImageNet
  pre-training (up to 81%), and training from scratch (up to 81%). Finally, we train
  a 307M parameter vision transformer on a massive collection of 4.5M images from
  the Internet and egocentric videos, and demonstrate clearly the benefits of scaling
  visual pre-training for robot learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: radosavovic23a
month: 0
tex_title: Real-World Robot Learning with Masked Visual Pre-training
firstpage: 416
lastpage: 426
page: 416-426
order: 416
cycles: false
bibtex_author: Radosavovic, Ilija and Xiao, Tete and James, Stephen and Abbeel, Pieter
  and Malik, Jitendra and Darrell, Trevor
author:
- given: Ilija
  family: Radosavovic
- given: Tete
  family: Xiao
- given: Stephen
  family: James
- given: Pieter
  family: Abbeel
- given: Jitendra
  family: Malik
- given: Trevor
  family: Darrell
date: 2023-03-06
address:
container-title: Proceedings of The 6th Conference on Robot Learning
volume: '205'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 3
  - 6
pdf: https://proceedings.mlr.press/v205/radosavovic23a/radosavovic23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
