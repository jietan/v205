---
title: Real-time Mapping of Physical Scene Properties with an Autonomous Robot Experimenter
section: Oral
openreview: r-w9Wh-QVnH
abstract: Neural fields can be trained from scratch to represent the shape and appearance
  of 3D scenes efficiently. It has also been shown that they can densely map correlated
  properties such as semantics, via sparse interactions from a human labeller. In
  this work, we show that a robot can densely annotate a scene with arbitrary discrete
  or continuous physical properties via its own fully-autonomous experimental interactions,
  as it simultaneously scans and maps it with an RGB-D camera. A variety of scene
  interactions are possible, including poking with force sensing to determine rigidity,
  measuring local material type with single-pixel spectroscopy or predicting force
  distributions by pushing. Sparse experimental interactions are guided by entropy
  to enable high efficiency, with tabletop scene properties densely mapped from scratch
  in a few minutes from a few tens of interactions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: haughton23a
month: 0
tex_title: Real-time Mapping of Physical Scene Properties with an Autonomous Robot
  Experimenter
firstpage: 118
lastpage: 127
page: 118-127
order: 118
cycles: false
bibtex_author: Haughton, Iain and Sucar, Edgar and Mouton, Andre and Johns, Edward
  and Davison, Andrew
author:
- given: Iain
  family: Haughton
- given: Edgar
  family: Sucar
- given: Andre
  family: Mouton
- given: Edward
  family: Johns
- given: Andrew
  family: Davison
date: 2023-03-06
address:
container-title: Proceedings of The 6th Conference on Robot Learning
volume: '205'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 3
  - 6
pdf: https://proceedings.mlr.press/v205/haughton23a/haughton23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
