---
title: 'R3M: A Universal Visual Representation for Robot Manipulation'
section: Poster
openreview: tGbpgz6yOrI
abstract: 'We study how visual representations pre-trained on diverse human video
  data can enable data-efficient learning of downstream robotic manipulation tasks.
  Concretely, we pre-train a visual representation using the Ego4D human video dataset
  using a combination of time-contrastive learning, video-language alignment, and
  an L1 penalty to encourage sparse and compact representations. The resulting representation,
  R3M, can be used as a frozen perception module for downstream policy learning. Across
  a suite of 12 simulated robot manipulation tasks, we find that R3M improves task
  success by over 20% compared to training from scratch and by over 10% compared to
  state-of-the-art visual representations like CLIP and MoCo. Furthermore, R3M enables
  a Franka Emika Panda arm to learn a range of manipulation tasks in a real, cluttered
  apartment given just 20 demonstrations. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nair23a
month: 0
tex_title: 'R3M: A Universal Visual Representation for Robot Manipulation'
firstpage: 892
lastpage: 909
page: 892-909
order: 892
cycles: false
bibtex_author: Nair, Suraj and Rajeswaran, Aravind and Kumar, Vikash and Finn, Chelsea
  and Gupta, Abhinav
author:
- given: Suraj
  family: Nair
- given: Aravind
  family: Rajeswaran
- given: Vikash
  family: Kumar
- given: Chelsea
  family: Finn
- given: Abhinav
  family: Gupta
date: 2023-03-06
address:
container-title: Proceedings of The 6th Conference on Robot Learning
volume: '205'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 3
  - 6
pdf: https://proceedings.mlr.press/v205/nair23a/nair23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
