---
title: 'Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from Demonstrations'
section: Oral
openreview: ndYsaoyzCWv
abstract: Learning from demonstration (LfD) has successfully solved tasks featuring
  a long time horizon. However, when the problem complexity also includes human-in-the-loop
  perturbations, state-of-the-art approaches do not guarantee the successful reproduction
  of a task. In this work, we identify the roots of this challenge as the failure
  of a learned continuous policy to satisfy the discrete plan implicit in the demonstration.
  By utilizing modes (rather than subgoals) as the discrete abstraction and motion
  policies with both mode invariance and goal reachability properties, we prove our
  learned continuous policy can simulate any discrete plan specified by a linear temporal
  logic (LTL) formula. Consequently, an imitator is robust to both task- and motion-level
  perturbations and guaranteed to achieve task success.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23a
month: 0
tex_title: 'Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from
  Demonstrations'
firstpage: 94
lastpage: 105
page: 94-105
order: 94
cycles: false
bibtex_author: Wang, Yanwei and Figueroa, Nadia and Li, Shen and Shah, Ankit and Shah,
  Julie
author:
- given: Yanwei
  family: Wang
- given: Nadia
  family: Figueroa
- given: Shen
  family: Li
- given: Ankit
  family: Shah
- given: Julie
  family: Shah
date: 2023-03-06
address:
container-title: Proceedings of The 6th Conference on Robot Learning
volume: '205'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 3
  - 6
pdf: https://proceedings.mlr.press/v205/wang23a/wang23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v205/wang23a/wang23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
