---
title: Visuo-Tactile Transformers for Manipulation
section: Poster
openreview: JqqSTgdQ85F
abstract: 'Learning representations in the joint domain of vision and touch can improve
  manipulation dexterity, robustness, and sample-complexity by exploiting mutual information
  and complementary cues. Here, we present Visuo-Tactile Transformers (VTTs), a novel
  multimodal representation learning approach suited for model-based reinforcement
  learning and planning. Our approach extends the Visual Transformer to handle visuo-tactile
  feedback. Specifically, VTT uses tactile feedback together with self and cross-modal
  attention to build latent heatmap representations that focus attention on important
  task features in the visual domain. We demonstrate the efficacy of VTT for representation
  learning with a comparative evaluation against baselines on four simulated robot
  tasks and one real world block pushing task. We conduct an ablation study over the
  components of VTT to highlight the importance of cross-modality in representation
  learning for robotic manipulation. '
software: https://github.com/yich7045/Visuo-Tactile-Transformers-for-Manipulation
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen23d
month: 0
tex_title: Visuo-Tactile Transformers for Manipulation
firstpage: 2026
lastpage: 2040
page: 2026-2040
order: 2026
cycles: false
bibtex_author: Chen, Yizhou and Merwe, Mark Van der and Sipos, Andrea and Fazeli,
  Nima
author:
- given: Yizhou
  family: Chen
- given: Mark Van der
  family: Merwe
- given: Andrea
  family: Sipos
- given: Nima
  family: Fazeli
date: 2023-03-06
address:
container-title: Proceedings of The 6th Conference on Robot Learning
volume: '205'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 3
  - 6
pdf: https://proceedings.mlr.press/v205/chen23d/chen23d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
